## 神经网络的优化算法
    反向传播算法和梯度下降算法
    梯度下降主要是用来优化单个参数的的取值
    而反向传播算法可以高校的在所有参数上面使用梯度下降算法 
    从而保证了训练数据上的损失函数尽可能的小
    梯度下降算法  
    参数sita 的梯度是偏导数值*a 来更新
    学习率设置的的过大导致不能在极值点进行收敛 
    学习率设置的台小虽然能保证收敛性 但是存在的问题是 增加了参数更新迭代的次数  也就是降低了优化的速度
    指数衰减方法来设置学习率的问题
    batch是一个很重要的概念 就是总的输入数据量 划分成多个batch  每次传一个batch的数据进行训练 这样避免大数据量下的内存溢出的问题
    
    模型训练过程中的过拟合问题的解决方法:
    避免过拟合的一个方法是正则化 regularization 就是在loss function中加入刻画模型复杂成都的指标 比如 L(x) + xB 其中B 刻画的就是模型的复杂程度
    L1 正则化是绝对值的和  
    L2 正则化是平方的和
    